{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-08T04:35:10.370019Z","iopub.execute_input":"2022-05-08T04:35:10.370343Z","iopub.status.idle":"2022-05-08T04:35:10.414119Z","shell.execute_reply.started":"2022-05-08T04:35:10.370263Z","shell.execute_reply":"2022-05-08T04:35:10.413316Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/fromsavedmodel/model.pth\n/kaggle/input/preprocessed-simpsons/processed_simpsons.csv\n/kaggle/input/spokenwordsworking/working_data_spoken_words.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nTo free up GPU: code below\nTo free up RAM: hit \"Factory Reset\" revolving logo in top right\n\"\"\"\n\n!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()  ","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:35:12.112293Z","iopub.execute_input":"2022-05-08T04:35:12.112645Z","iopub.status.idle":"2022-05-08T04:35:26.599450Z","shell.execute_reply.started":"2022-05-08T04:35:12.112572Z","shell.execute_reply":"2022-05-08T04:35:26.598556Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=43f6875a217fa64e3831503d4e1f13debb190ca8535e9439f2f1aa0b1e7775bc\n  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mInitial GPU Usage\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  0% |\nGPU Usage after emptying the cache\n| ID | GPU | MEM |\n------------------\n|  0 |  9% |  2% |\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n\n\n######################\n# all the imports\n\nimport glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm.notebook import tqdm, trange\n\nfrom pathlib import Path\n\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n    \n    \n    \n# data = pd.read_csv(\"../input/the-simpsons-dataset/simpsons_script_lines.csv\")\ndata = pd.read_csv(\"../input/preprocessed-simpsons/processed_simpsons.csv\") # best so far\n# {'perplexity_': tensor(7.1370)}\n# data = pd.read_csv(\"../input/spokenwordsworking/working_data_spoken_words.csv\") # NORMALIZED TEXT\n# ^^^ BAD RESULTS(seemed worse -- a lot worse)!!! {'perplexity_': tensor(7.8950)}\n\n\nCHARACTER_NAME = \"Homer Simpson\"\ncontexted = []\n\n# context window of size 7\nn = 7\n\nfor i in data[data.character == CHARACTER_NAME].index:\n  if i < n:\n    continue\n  row = []\n  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n  for j in range(i, prev, -1):\n    row.append(data.line[j])\n  contexted.append(row)\n\ncolumns = ['response', 'context'] \ncolumns = columns + ['context/' + str(i) for i in range(n - 1)]\n\ndf = pd.DataFrame.from_records(contexted, columns=columns)\nprint(df.sample(6))\n\n\ntrn_df, val_df = train_test_split(df, test_size=0.1)\ntrn_df.head()\n######################\n\n\n# create dataset suitable for our model\ndef construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.examples = []\n            for _, row in df.iterrows():\n                conv = construct_conv(row, tokenizer)\n                self.examples.append(conv)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)\n    \n######################\n# Cacheing and storing of data/checkpoints\n\ndef load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)\n        \n        \n######################\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n\n\nprint(\"[INFO] simulating chat before fine-tuning....\")\n# Let's chat for 5 lines\n# for step in range(5):\n#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n#     new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n\n#     # append the new user input tokens to the chat history\n#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n#     # generated a response while limiting the total chat history to 1000 tokens, \n#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n\n#     # pretty print last ouput tokens from bot\n#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n\n######################\n\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\nusing a masked language modeling (MLM) loss.\n\"\"\"\n\n# Configs\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\n######################\n# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = 'output-small'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft/DialoGPT-small'\n        self.config_name = 'microsoft/DialoGPT-small'\n        self.tokenizer_name = 'microsoft/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 4\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n\nargs = Args()\n\n\n######################\ndef train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n    # add_special_tokens_(model, tokenizer)\n\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n        args.model_name_or_path\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step\n\n# Evaluation of some model\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n\n    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n    os.makedirs(eval_output_dir, exist_ok=True)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result\n\n######################\n# Main runner\n\ndef main(df_trn, df_val):\n    args = Args()\n    \n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n        and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    # Set seed\n    set_seed(args)\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelWithLMHead.from_pretrained(\n        args.model_name_or_path,\n        from_tf=False,\n        config=config,\n        cache_dir=args.cache_dir,\n    )\n    model.to(args.device)\n    \n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    if args.do_train:\n        # Create output directory if needed\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results\n\n\n######################\nmain(trn_df, val_df)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:35:44.902949Z","iopub.execute_input":"2022-05-08T04:35:44.903429Z","iopub.status.idle":"2022-05-08T05:39:21.326946Z","shell.execute_reply.started":"2022-05-08T04:35:44.903375Z","shell.execute_reply":"2022-05-08T05:39:21.325354Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"                                                response  \\\n25973                     (WORRIED) How did we get here?   \n13502   (CHECKING FOOD) Okay, cranberry sauce, stuffi...   \n10189                       (PROUDLY) God bless America!   \n25172                               Flushy! Go get help!   \n13041   I mean I knew scientists wasted their lives, ...   \n11571   Check it out. There's Island Blackjack, Islan...   \n\n                                                 context  \\\n25973              Oh baby, that's all I needed to hear.   \n13502                   It's the law. My hands are tied.   \n10189                             (TOUCHING SPRING) Ow!!   \n25172   (DOPPLER) Dohhh! (QUICKLY) Sauce, cheese, top...   \n13041                                   Possibly, but...   \n11571                       INT. casino - moments later)   \n\n                                               context/0  \\\n25973       Well, I was trying to buy you snack cakes...   \n13502   It's not fair, Dad. Why should an animal die ...   \n10189   (WAVING SPRING) It's fun for the whole family...   \n25172   (RASPY) Dough, sauce, cheese, topping. Dough,...   \n13041        You must be the most boring woman on earth.   \n11571                                   It could be you!   \n\n                                               context/1  \\\n25973   Honey... just what was going on with you and ...   \n13502                           Ext. woods - CONTINUOUS)   \n10189                                    Good work, Dad.   \n25172                                 EXT. street - day)   \n13041   Every day I get up at five-thirty, watch the ...   \n11571                                \"The Lucky Savage\"?   \n\n                                               context/2  \\\n25973                               That exists? Oh boy.   \n13502   EXT. wide shot of WOODS - THE NEXT DAY - ESTA...   \n10189                        (SINCERE) Hey, that's cute.   \n25172   (LOOKS AT MYPAD) So, that's how you make a pi...   \n13041                           (CONTENTED CHIMP NOISES)   \n11571   (BRIGHT) Anyhoo, I'm about to share with you ...   \n\n                                               context/3  \\\n25973   Of course I do. Now let's go home and spend a...   \n13502                          (GRITTED TEETH) Lemons...   \n10189             Those aren't the dog's eyes, are they?   \n25172                     Dough, sauce, cheese, topping.   \n13041                   Ext. chimp refuge - day - later)   \n11571   A plane! (CALLING) Stop! Get me off this stin...   \n\n                                               context/4  \\\n25973   That was you? I mean, (STAMMERS) it was wonde...   \n13502                                  Hunting? Dad, no!   \n10189   Knowing you always hate my first idea, I prep...   \n25172            (STUNNED) How long have I been playing?   \n13041   (CHANGING SUBJECT) I-I love what you've done ...   \n11571   Friends -- when I came here one month ago, th...   \n\n                                               context/5  \n25973   Look at them, coochie cooing like that time o...  \n13502   Which I'll be doing tomorrow morning, smart guy.  \n10189                     Just for the prototype, honey.  \n25172   (PLAYING) Dough, sauce, cheese, topping. Doug...  \n13041                Could we talk about something else?  \n11571        EXT. TROPICAL ISLAND - ROCKY HILLTOP - day)  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59461ae41ab743138a80588aa0f9fa0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae780e6acff49a4ab307eba22c7bc20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7dbb4b89d7140ce878889b7cf0294d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2ffdf6aaa44841a7284c9478346b56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45bd55a375fd4a2896d3ad3eb9893538"}},"metadata":{}},{"name":"stdout","text":"[INFO] simulating chat before fine-tuning....\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79fc85b72498435eb27ba981d40b95d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd780c12a524cd2a334a7df3b66453b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e0c6b236b24ac082e4922a7e9ee205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334f20f74c474f2482574266519dfea5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:911: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98bdda26e20543c5a08f2a7d9d72b213"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35aa31d74c5247eea6ae957b1924eb38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6714 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f00fe7be74f34b7892ba33d1201d4daa"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6714 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a226610e0058480e9d350873a2eb939f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6714 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29987c954b25486ab16eb7e117f6fa65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6714 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d05ba611a0be425fa11ebd42f9fffd19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/746 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdfe365c1c0f4da3b8113d602ffaf182"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'perplexity_': tensor(7.1370)}"},"metadata":{}}]},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n# %load_ext tensorboard\n# %tensorboard --logdir ./runs\n# %reload_ext tensorboard\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:35:35.235769Z","iopub.status.idle":"2022-05-08T04:35:35.236370Z","shell.execute_reply.started":"2022-05-08T04:35:35.236121Z","shell.execute_reply":"2022-05-08T04:35:35.236148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, './best_so_far.pth')","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:45:24.157537Z","iopub.execute_input":"2022-05-08T05:45:24.158267Z","iopub.status.idle":"2022-05-08T05:45:26.100487Z","shell.execute_reply.started":"2022-05-08T05:45:24.158231Z","shell.execute_reply":"2022-05-08T05:45:26.096189Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\nmodel = AutoModelWithLMHead.from_pretrained('./output-small')\n# model = torch.load('./best_so_far.pth')\n\nprint(tokenizer)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:39:22.043906Z","iopub.execute_input":"2022-05-08T05:39:22.046474Z","iopub.status.idle":"2022-05-08T05:39:27.801032Z","shell.execute_reply.started":"2022-05-08T05:39:22.046408Z","shell.execute_reply":"2022-05-08T05:39:27.800270Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"PreTrainedTokenizerFast(name_or_path='microsoft/DialoGPT-small', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"HomerBotSpokenWords: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:39:27.802871Z","iopub.execute_input":"2022-05-08T05:39:27.803130Z","iopub.status.idle":"2022-05-08T05:40:33.494231Z","shell.execute_reply.started":"2022-05-08T05:39:27.803096Z","shell.execute_reply":"2022-05-08T05:40:33.493481Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdin","text":">> User: Hi there! Where are you? Any chance of skyping?\n"},{"name":"stdout","text":"HomerBotSpokenWords:  (CHUCKLES) No, I'm not gonna skyp.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: Why not? You look great on camera\n"},{"name":"stdout","text":"HomerBotSpokenWords:  (SADLY) I'm afraid I won't skyp the party.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: What an interesting response.....I can't tell if you're trying to trick me\n"},{"name":"stdout","text":"HomerBotSpokenWords:  (SHORT SCREAM)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: haha you're a funny guy homer you know that?\n"},{"name":"stdout","text":"HomerBotSpokenWords:  Yes, but you're no fun.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"chatt again....","metadata":{}},{"cell_type":"code","source":"# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"HomerBotSpokenWords: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:45:52.645552Z","iopub.execute_input":"2022-05-08T05:45:52.646124Z","iopub.status.idle":"2022-05-08T05:47:46.258047Z","shell.execute_reply.started":"2022-05-08T05:45:52.646091Z","shell.execute_reply":"2022-05-08T05:47:46.257299Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdin","text":">> User: why won't my laptop hold a charge?\n"},{"name":"stdout","text":"HomerBotSpokenWords:  (STUNNED) That's the old \"shut up\" button.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: right? I mean can't that elon guy put a tesla in one of these things yet?\n"},{"name":"stdout","text":"HomerBotSpokenWords:  You can't.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: what do you mean by that\n"},{"name":"stdout","text":"HomerBotSpokenWords:  (CONFUSED) I mean, what are you gonna do with it?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: what would you do with it?\n"},{"name":"stdout","text":"HomerBotSpokenWords:  I'd sell it to a corporation for a million dollars.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer_pre = AutoTokenizer.from_pretrained(\"lidiya/bart-base-samsum\")\n\nmodel_pre = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-base-samsum\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:43:25.872890Z","iopub.execute_input":"2022-05-08T05:43:25.874844Z","iopub.status.idle":"2022-05-08T05:44:00.553332Z","shell.execute_reply.started":"2022-05-08T05:43:25.874795Z","shell.execute_reply":"2022-05-08T05:44:00.552588Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1bb59193ea940d081f3bbfe69ed964d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.64k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ffb27840f0944df8d806ce6facd30c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc7b11537774ff296412aa875c8dbe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbb89d440fe64afc868923fcd920b53c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c094e131ce4a018e9602d014a68a11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27264c9aac8f4be68abbb090e5fc40f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/532M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"003b19ba564646488029dd9a7cde2be0"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import pipeline\n\ndialog_summarizer = pipeline(\"summarization\", model=model_pre, tokenizer=tokenizer_pre)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:44:22.691982Z","iopub.execute_input":"2022-05-08T05:44:22.692235Z","iopub.status.idle":"2022-05-08T05:44:25.406068Z","shell.execute_reply.started":"2022-05-08T05:44:22.692207Z","shell.execute_reply":"2022-05-08T05:44:25.405347Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"\n>> User: Hi there! Where are you? Any chance of skyping?\nHomerBotSpokenWords:  (CHUCKLES) No, I'm not gonna skyp.\n>> User: Why not? You look great on camera\nHomerBotSpokenWords:  (SADLY) I'm afraid I won't skyp the party.\n>> User: What an interesting response.....I can't tell if you're trying to trick me\nHomerBotSpokenWords:  (SHORT SCREAM)\n>> User: haha you're a funny guy homer you know that?\nHomerBotSpokenWords:  Yes, but you're no fun.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:44:25.407733Z","iopub.execute_input":"2022-05-08T05:44:25.408208Z","iopub.status.idle":"2022-05-08T05:44:25.414932Z","shell.execute_reply.started":"2022-05-08T05:44:25.408171Z","shell.execute_reply":"2022-05-08T05:44:25.414322Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"result = dialog_summarizer(text, min_length=10, max_length=40)\nresult[0]['summary_text']\n# trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:44:25.423175Z","iopub.execute_input":"2022-05-08T05:44:25.423656Z","iopub.status.idle":"2022-05-08T05:44:27.315071Z","shell.execute_reply.started":"2022-05-08T05:44:25.423622Z","shell.execute_reply":"2022-05-08T05:44:27.314419Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"\"HomerBotSpokenWords won't skyp the party.\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ndialog_summarizer = pipeline(\"summarization\", model=model_pre, tokenizer=tokenizer_pre)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:49:09.285955Z","iopub.execute_input":"2022-05-08T05:49:09.286205Z","iopub.status.idle":"2022-05-08T05:49:09.290323Z","shell.execute_reply.started":"2022-05-08T05:49:09.286178Z","shell.execute_reply":"2022-05-08T05:49:09.289556Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"\n>> User: why won't my laptop hold a charge?\nHomerBotSpokenWords:  (STUNNED) That's the old \"shut up\" button.\n>> User: right? I mean can't that elon guy put a tesla in one of these things yet?\nHomerBotSpokenWords:  You can't.\n>> User: what do you mean by that\nHomerBotSpokenWords:  (CONFUSED) I mean, what are you gonna do with it?\n>> User: what would you do with it?\nHomerBotSpokenWords:  I'd sell it to a corporation for a million dollars.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:49:10.356278Z","iopub.execute_input":"2022-05-08T05:49:10.357070Z","iopub.status.idle":"2022-05-08T05:49:10.361544Z","shell.execute_reply.started":"2022-05-08T05:49:10.357028Z","shell.execute_reply":"2022-05-08T05:49:10.360818Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"result = dialog_summarizer(text, min_length=10, max_length=40)\nresult[0]['summary_text']\n# trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T05:49:12.195866Z","iopub.execute_input":"2022-05-08T05:49:12.196350Z","iopub.status.idle":"2022-05-08T05:49:14.071762Z","shell.execute_reply.started":"2022-05-08T05:49:12.196308Z","shell.execute_reply":"2022-05-08T05:49:14.070876Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'HomerBotSpokenWords will sell his laptop to a corporation for a million dollars.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BELOW WAS DONE WITH THE ORIGINAL DATASET (../input/preprocessed-simpsons) THAT CONTAINED THE EG -> QUOTES (LAUGHS) \"Something Homer said.\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T02:59:55.315010Z","iopub.status.idle":"2022-05-08T02:59:55.315657Z","shell.execute_reply.started":"2022-05-08T02:59:55.315380Z","shell.execute_reply":"2022-05-08T02:59:55.315409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n# model = AutoModelWithLMHead.from_pretrained('./output-small')\nmodel = torch.load('../input/fromsavedmodel/model.pth')\n\nprint(tokenizer)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:35:05.007397Z","iopub.execute_input":"2022-05-08T00:35:05.007667Z","iopub.status.idle":"2022-05-08T00:35:09.186984Z","shell.execute_reply.started":"2022-05-08T00:35:05.007637Z","shell.execute_reply":"2022-05-08T00:35:09.185259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"HomerBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:35:14.235299Z","iopub.execute_input":"2022-05-08T00:35:14.235544Z","iopub.status.idle":"2022-05-08T00:35:50.139743Z","shell.execute_reply.started":"2022-05-08T00:35:14.235517Z","shell.execute_reply":"2022-05-08T00:35:50.138889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets chat for 10 lines now instead of 5","metadata":{}},{"cell_type":"code","source":"# torch.save(model.state_dict(), 'homer_model_weights.pth')\n# torch.save(model, 'model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seem to get different results when I run it again?","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n# model = AutoModelWithLMHead.from_pretrained('./output-small')\nmodel = torch.load('../input/fromsavedmodel/model.pth')\n\n# print(tokenizer)\n# print(model)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:37:20.83659Z","iopub.execute_input":"2022-05-08T00:37:20.837338Z","iopub.status.idle":"2022-05-08T00:37:24.956131Z","shell.execute_reply.started":"2022-05-08T00:37:20.837302Z","shell.execute_reply":"2022-05-08T00:37:24.955396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try it out","metadata":{}},{"cell_type":"code","source":"# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"BartBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:38:48.896865Z","iopub.execute_input":"2022-05-08T00:38:48.897449Z","iopub.status.idle":"2022-05-08T00:39:44.073583Z","shell.execute_reply.started":"2022-05-08T00:38:48.897407Z","shell.execute_reply":"2022-05-08T00:39:44.07283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Summarized","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer_pre = AutoTokenizer.from_pretrained(\"lidiya/bart-base-samsum\")\n\nmodel_pre = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-base-samsum\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:39:50.553547Z","iopub.execute_input":"2022-05-08T00:39:50.553823Z","iopub.status.idle":"2022-05-08T00:39:56.565437Z","shell.execute_reply.started":"2022-05-08T00:39:50.553792Z","shell.execute_reply":"2022-05-08T00:39:56.5644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\ndialog_summarizer = pipeline(\"summarization\", model=model_pre, tokenizer=tokenizer_pre)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:39:56.571231Z","iopub.execute_input":"2022-05-08T00:39:56.573446Z","iopub.status.idle":"2022-05-08T00:39:56.601674Z","shell.execute_reply.started":"2022-05-08T00:39:56.573404Z","shell.execute_reply":"2022-05-08T00:39:56.600788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"\n>> User: what do you think about elon\nBartBot:  I don't know. I don' think he's a bad guy. He's just...\n>> User: okay, we will stay away from politics whats your favorite ice cream\nBartBot:  (SAD) I'll just lie down and wait for the kids to get home.\n>> User: no help me learn english would ya\nBartBot:  int. simpson house - living room - day)\n>> User: hmmm\nBartBot:  I'll take that as a yes.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:40:07.322619Z","iopub.execute_input":"2022-05-08T00:40:07.322944Z","iopub.status.idle":"2022-05-08T00:40:07.328236Z","shell.execute_reply.started":"2022-05-08T00:40:07.32291Z","shell.execute_reply":"2022-05-08T00:40:07.327252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text = raw_datasets[\"test\"][100]['dialogue']\ntext = \"\"\"\nUser: whats your name\nBartBot:  I'm Homer Simpson.\nUser: are you a hunter?\nBartBot:  No.\nUser: where do you live?\nBartBot:  I don't know. I'm not really sure.\nUser: are you lying to me?\nBartBot: !!!!.!!!!!!\n\"\"\"\n\n# text = \"\"\"\n# >> User: hey there! how are you?\n# BartBot:  I'm fine.\n# >> User: what have you been up to today?\n# BartBot:  (TOUCHED) Really? You think I'm so cool?\n# >> User: not really, just wondering what you've been doing\n# BartBot:  (CALMLY) I've been in a lot of places, but I can't describe.\n# >> User: why not?\n# BartBot:  Well, it's a lot like a prison, but it's not so bad.\n# \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:36:53.865242Z","iopub.execute_input":"2022-05-08T00:36:53.865506Z","iopub.status.idle":"2022-05-08T00:36:53.869336Z","shell.execute_reply.started":"2022-05-08T00:36:53.865478Z","shell.execute_reply":"2022-05-08T00:36:53.868686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = dialog_summarizer(text, min_length=10, max_length=40)\nresult[0]['summary_text']\n# trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:40:11.682279Z","iopub.execute_input":"2022-05-08T00:40:11.682529Z","iopub.status.idle":"2022-05-08T00:40:13.708377Z","shell.execute_reply.started":"2022-05-08T00:40:11.682501Z","shell.execute_reply":"2022-05-08T00:40:13.707606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer_pre = AutoTokenizer.from_pretrained(\"lidiya/bart-base-samsum\")\n\nmodel_pre = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-base-samsum\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:40:18.663062Z","iopub.execute_input":"2022-05-08T00:40:18.663498Z","iopub.status.idle":"2022-05-08T00:40:24.49568Z","shell.execute_reply.started":"2022-05-08T00:40:18.66346Z","shell.execute_reply":"2022-05-08T00:40:24.494906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"\"\"\n>> User: hey there! this is pretty cool to meet you\nBartBot:  (TENTATIVE) Hey, I'm Homer Simpson.\n>> User: Oh I know that, I'm just using you to learn english\nBartBot:  Oh, right.\n>> User: what do you think about that? should you be used in this way\nBartBot:  Well, I think I'm gonna be a better father.\n>> User: what makes you think that? aren't you pretty stupid?\nBartBot:  (ANNOYED) I'm not stupid. I'm a decent father. I've learned a lot from you.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:40:25.536981Z","iopub.execute_input":"2022-05-08T00:40:25.537256Z","iopub.status.idle":"2022-05-08T00:40:25.541645Z","shell.execute_reply.started":"2022-05-08T00:40:25.537227Z","shell.execute_reply":"2022-05-08T00:40:25.540859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = dialog_summarizer(text, min_length=10, max_length=40)\nresult[0]['summary_text']\n# trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T00:40:26.446446Z","iopub.execute_input":"2022-05-08T00:40:26.446722Z","iopub.status.idle":"2022-05-08T00:40:27.914774Z","shell.execute_reply.started":"2022-05-08T00:40:26.446693Z","shell.execute_reply":"2022-05-08T00:40:27.913948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}